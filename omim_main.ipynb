{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TexAnASD - Text Analytics for ASD Risk Gene Predictions\n",
    "- IEEE BIBM 2019 - Workshop on Machine Learning and Artificial Intelligence in Bioinformatics and Medical Informatics\n",
    "- San Diego, CA, USA, November 18 - 21, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "import json                  # For reading config. / and managing objects\n",
    "import ast\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "from omim import generateOMIM_ids\n",
    "from omim import readOmimDataFromWeb, processOmimDataFiles\n",
    "\n",
    "from omim_pipeline import plotROC, plotROC_Model\n",
    "from omim_pipeline import shuffleData, cross_validation_split\n",
    "from omim_pipeline import runPipeline,runParallelCrossValidation\n",
    "from omim_pipeline import runModelTrainTest\n",
    "\n",
    "from auxiliary_func import save_obj, load_obj, read_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "CONST_MODE_SEQUENTIAL = 0\n",
    "CONST_MODE_PARALLEL   = 1\n",
    " \n",
    "config = read_config('./config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = config[\"app\"][\"LOG_DIR\"]+\"omim_\"+RUN_ID+\".log\"\n",
    "\n",
    "OMIM_PATH           = config[\"omim\"][\"OMIM_PATH\"]\n",
    "OMIM_GENE_PATH      = config[\"omim\"][\"OMIM_GENE_PATH\"]\n",
    "OMIM_PROC_PATH      = config[\"omim\"][\"OMIM_PROC_PATH\"]\n",
    "READ_OMIM_FROM_WEB  = ast.literal_eval(config[\"omim\"][\"READ_OMIM_FROM_WEB\"])\n",
    "READ_OMIM_FROM_FILE = ast.literal_eval(config[\"omim\"][\"READ_OMIM_FROM_FILE\"])\n",
    "\n",
    "DATA_PATH          = config[\"app\"][\"DATA_PATH\"]\n",
    "PROJECT_DATA       = config[\"app\"][\"PROJECT_DATA\"]\n",
    "OMIM_IDS_FILE      = config[\"app\"][\"OMIM_IDS_FILE\"]\n",
    "GENERATE_OMIM_IDS  = ast.literal_eval(config[\"app\"][\"GENERATE_OMIM_IDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"omim\"][\"LIMIT_SAMPLES_NO\"]<=0:\n",
    "    LIMIT_SAMPLE_NUMBER = None\n",
    "else:\n",
    "    LIMIT_SAMPLE_NUMBER = config[\"omim\"][\"LIMIT_SAMPLES_NO\"]\n",
    "\n",
    "print(\"Using {} as sample limit for the training model.\".format(LIMIT_SAMPLE_NUMBER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=LOG, \n",
    "                    format=config[\"app\"][\"LOG_FORMAT\"], \n",
    "                    level=config[\"app\"][\"LOG_LEVEL\"], \n",
    "                    datefmt=config[\"app\"][\"LOG_DATE_FORMAT\"]\n",
    "                   )\n",
    "logging.info('Started Run ID:'+RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Genes ID and Mappings\n",
    "String database and *forecASD* use protein ids. To be able to compare we need to restrict to the same genes for those proteins. We also need the same mappings from entrez to omim ids to retrieve the data from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_OMIM_IDS:\n",
    "#     print(\"Generating the new omim mapping file...\")\n",
    "    logging.info(\"Generating the new omim mapping file...\")\n",
    "    df_req_omim_ids = generateOMIM_ids(config[\"omim\"][\"OMIM_TO_GENE_FILE\"], config[\"stringdb\"][\"PROTEIN_INFO_FILE\"], \n",
    "                                       config[\"sfari\"][\"SFARI_GENE_FILE\"], OMIM_IDS_FILE)\n",
    "else:\n",
    "#     print(\"Using previously generated OMIM_ID list.\")\n",
    "    logging.info(\"Using previously generated OMIM_ID list.\")\n",
    "    df_req_omim_ids = pd.read_csv(OMIM_IDS_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve and Process OMIM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Gene list to retrieve data from OMIM\n",
    "\n",
    "### Data Columns:\n",
    "- **HGNC**:          HUGO Gene Nomenclature Committee gene name\n",
    "- **Entrez_ID**:     Entrez Id number\n",
    "- **OMIM_ID**:       Omim id number\n",
    "- **ENSEMBL_ID**:    Ensembl Gene Id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omim_ids = pd.read_csv(OMIM_IDS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read OMIM Data From the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API summary\n",
    "```\n",
    "https://api.omim.org/api/entry/search?search=approved_gene_symbol:ARF5&apiKey=<apiKey>&format=json\n",
    "https://api.omim.org/api/entry?mimNumber=<omimid>&apiKey=<apiKey>&format=json&include=all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_OMIM_FROM_WEB:\n",
    "    logging.info(\"Reading Omim from web.\")\n",
    "    readOmimDataFromWeb(df_omim_ids, OMIM_GENE_PATH, config )\n",
    "else:\n",
    "    #print(\"Using previously downloaded files(\"+str(len(os.listdir(OMIM_GENE_PATH)))+\").\")\n",
    "    logging.info(\"Using previously downloaded files(\"+str(len(os.listdir(OMIM_GENE_PATH)))+\").\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process OMIM Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not READ_OMIM_FROM_FILE:\n",
    "    logging.info(\"Processing OMIM files.\")\n",
    "    df_omim_info = processOmimDataFiles(df_omim_ids, OMIM_GENE_PATH, OMIM_PROC_PATH+\"/omim_summary.csv\")\n",
    "else:\n",
    "    logging.info(\"Using previously computed features.\")\n",
    "    df_omim_info = pd.read_csv(OMIM_PROC_PATH+\"/omim_summary.csv\")\n",
    "\n",
    "print()\n",
    "print(\"Number of Genes from Omim Data: {}. \\nNumber of features for each gene: {}\".format(df_omim_info.shape[0],df_omim_info.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Text with Class and saving to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omimtext = df_omim_info[[\"gene\",\"text\"]].merge(df_omim_ids, left_on=\"gene\",right_on=\"HGNC\")[[\"gene\",\"text\",\"ASD\"]]\n",
    "logging.info(\"Saving omim info and class to file...\")\n",
    "df_omimtext.to_csv(OMIM_PROC_PATH+\"/omim_text_and_class.csv\",index=False)\n",
    "logging.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split Train and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- previous work results (to compare AUC and plot ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Reading forecASD results.\")\n",
    "df_forecASD_results = pd.read_csv(config[\"forecasd\"][\"results_file\"])\n",
    "df_forecASD_results = pd.read_csv(\"./data/forecASD/forecASD_table.csv\") # Data generated by running the code.\n",
    "df_forecASD_results = df_forecASD_results[[\"ensembl_string\",\"forecASD\",\"STRING_score\",\"BrainSpan_score\",\"krishnan_post\",\"Netscore\",\"rASD\",\"SFARI_listed\",]].copy()\n",
    "logging.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- previous work test data (obtaining by saving temporal objects into files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Reading forecASD test data.\")\n",
    "df_forecASD_test      = pd.read_csv(config[\"forecasd\"][\"test_file\"])\n",
    "df_forecASD_test_cols = df_forecASD_test.columns.tolist(); df_forecASD_test_cols[0]=\"ENSP\"; df_forecASD_test.columns=df_forecASD_test_cols\n",
    "df_forecASD_test      = df_forecASD_test[[\"ENSP\"]].copy()\n",
    "logging.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- previous work results for test data only.\n",
    "\n",
    "We consider test data that we are using as test data. This will allow us to compute a new AUC-ROC and be able to compare our accuracy and the previous works accurace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecASD_test_results = df_forecASD_results.merge(df_req_omim_ids, left_on=\"ensembl_string\", right_on=\"ENSP\")[[\"ENSP\",\"HGNC\",\n",
    "                                                                                                                  \"forecASD\",\"STRING_score\",\"BrainSpan_score\",\"krishnan_post\",\n",
    "                                                                                                                  \"SFARI_listed\",\"ASD\"]]\n",
    "\n",
    "df_forecASD_test_results = df_forecASD_test_results.merge(df_forecASD_test).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"ForecASD test shape {}\".format(df_forecASD_test.shape))\n",
    "logging.info(\"ForecASD test results shape {}\".format(df_forecASD_test_results.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proteins list in test and not in test in previous works.\n",
    "\n",
    "We can use these lists for train and test purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Computing train set...\")\n",
    "lst_test_values_notin_test_results = [i for i in df_omimtext[\"gene\"].tolist() if i not in df_forecASD_test_results[\"HGNC\"].tolist()]\n",
    "lst_test_values_notin_test_results = list(set(lst_test_values_notin_test_results))\n",
    "logging.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Computing test set...\")\n",
    "lst_test_values_in_test_results = [i for i in df_omimtext[\"gene\"].tolist() if i in df_forecASD_test_results[\"HGNC\"].tolist()]\n",
    "lst_test_values_in_test_results = list(set(lst_test_values_in_test_results))\n",
    "logging.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"The number of samples usable for test purposes is {}\".format(len(lst_test_values_in_test_results)))\n",
    "logging.info(\"The number of samples usable for train purposes is {}\".format(len(lst_test_values_notin_test_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Generating train and test samples.\")\n",
    "df_test_lst  = pd.DataFrame(lst_test_values_in_test_results,columns=[\"gene\"])\n",
    "df_train_lst = pd.DataFrame(lst_test_values_notin_test_results,columns=[\"gene\"])\n",
    "df_test_samples  = df_omimtext.merge(df_test_lst)\n",
    "df_train_samples = df_omimtext.merge(df_train_lst)\n",
    "\n",
    "# Limiting if argument is set.\n",
    "df_test_samples  = df_test_samples[:LIMIT_SAMPLE_NUMBER]\n",
    "df_train_samples = df_train_samples[:LIMIT_SAMPLE_NUMBER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"app\"][\"SAVE_TRAIN_TEST_DF\"]:\n",
    "    df_train_samples[[\"gene\",'ASD']].to_csv(PROJECT_DATA+\"/train_samples_labels.csv\")\n",
    "    df_test_samples[[\"gene\",'ASD']].to_csv(PROJECT_DATA+\"/test_samples_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_tmp = df_train_samples.copy()\n",
    "df_ts_tmp = df_test_samples.copy()\n",
    " \n",
    "# Creates two subplots and unpacks the output array immediately\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, figsize=(15,5))\n",
    "\n",
    "ax1.bar([\"Class 0\"],df_tr_tmp[df_tr_tmp[\"ASD\"]==0].count()[0], label=\"Negative\")\n",
    "ax1.bar([\"Class 1\"],df_tr_tmp[df_tr_tmp[\"ASD\"]==1].count()[0], label=\"Positive\")\n",
    "\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "ax1.set_title('Train Split Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.bar([\"Class 0\"],df_ts_tmp[df_ts_tmp[\"ASD\"]==0].count()[0], label=\"Negative\")\n",
    "ax2.bar([\"Class 1\"],df_ts_tmp[df_ts_tmp[\"ASD\"]==1].count()[0], label=\"Positive\")\n",
    "\n",
    "ax2.set_ylabel('Number of Samples')\n",
    "ax2.set_title('Test Split Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "try: os.makedirs(\"./images\")\n",
    "except: pass\n",
    "plt.savefig(\"./images/train_test_distribution_2.eps\", format=\"eps\",dpi=1200,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_cross_validation_split():\n",
    "    global train_samples, train_labels, test_samples, test_labels, data_folds\n",
    "    \n",
    "    if config[\"app\"][\"COMPUTE_SPLITS\"]:\n",
    "        logging.info(\"Computing Train/Test-corpus/class Splits\")\n",
    "        arr_train_samples = df_train_samples[\"text\"].values\n",
    "        arr_train_labels  = df_train_samples[\"ASD\"].values\n",
    "\n",
    "        arr_test_samples  = df_test_samples[\"text\"].values\n",
    "        arr_test_labels   = df_test_samples[\"ASD\"].values\n",
    "\n",
    "        train_samples, train_labels = shuffleData(arr_train_samples, arr_train_labels)\n",
    "        test_samples,  test_labels  = shuffleData(arr_test_samples,  arr_test_labels)\n",
    "\n",
    "        data_folds = cross_validation_split(train_samples, train_labels, k_folds=config[\"app\"][\"K_FOLDS\"])\n",
    "\n",
    "        for i in range(len(data_folds)):\n",
    "            tmp_no_samples      = len(data_folds[i][\"labels\"])\n",
    "            tmp_pos_samples     = np.array(data_folds[i][\"labels\"]).sum()\n",
    "            tmp_pos_samples_pct = np.round(tmp_pos_samples/tmp_no_samples*100,2)\n",
    "\n",
    "            logging.info(\"{}-Fold length is {} samples and {} labels. {:3d} ({:3.2f}%) positive samples\".format( i, len(data_folds[i][\"data\"]), \n",
    "                                                                                                                tmp_no_samples,tmp_pos_samples,tmp_pos_samples_pct))\n",
    "\n",
    "\n",
    "        save_obj(train_samples, PROJECT_DATA+\"/computed_data/\"+ \"train_samples\"   +\".pkl\")\n",
    "        save_obj(test_samples,  PROJECT_DATA+\"/computed_data/\"+ \"test_samples\"    +\".pkl\")\n",
    "        save_obj(train_labels,  PROJECT_DATA+\"/computed_data/\"+ \"train_labels\"    +\".pkl\")\n",
    "        save_obj(test_labels,   PROJECT_DATA+\"/computed_data/\"+ \"test_labels\"     +\".pkl\")\n",
    "\n",
    "        for i in range(len(data_folds)):\n",
    "            save_obj(data_folds[i], PROJECT_DATA+\"/computed_data/\"+ \"fold_{:02d}_data\".format(i) +\".pkl\")\n",
    "\n",
    "    else:\n",
    "        logging.info(\"READING Train/Test-corpus/class Splits from files.\")\n",
    "\n",
    "        train_samples = load_obj(PROJECT_DATA+\"/computed_data/\"+ \"train_samples\"   +\".pkl\")\n",
    "        test_samples  = load_obj(PROJECT_DATA+\"/computed_data/\"+ \"test_samples\"    +\".pkl\")\n",
    "        train_labels  = load_obj(PROJECT_DATA+\"/computed_data/\"+ \"train_labels\"    +\".pkl\")\n",
    "        test_labels   = load_obj(PROJECT_DATA+\"/computed_data/\"+ \"test_labels\"     +\".pkl\")\n",
    "\n",
    "        data_folds = []\n",
    "        for i in range(config[\"app\"][\"K_FOLDS\"]):\n",
    "            tmp_i_fold = load_obj(PROJECT_DATA+\"/computed_data/\"+ \"fold_{:02d}_data\".format(i) +\".pkl\")\n",
    "            data_folds.append(tmp_i_fold)\n",
    "\n",
    "            tmp_no_samples      = len(tmp_i_fold[\"labels\"])\n",
    "            tmp_pos_samples     = np.array(tmp_i_fold[\"labels\"]).sum()\n",
    "            tmp_pos_samples_pct = np.round(tmp_pos_samples/tmp_no_samples*100,2)\n",
    "\n",
    "            logging.info(\"{}-Fold length is {} samples and {} labels. {:3d} ({:3.2f}%) positive samples\".format( i, len(tmp_i_fold[\"data\"]), \n",
    "                                                                                                                tmp_no_samples,tmp_pos_samples,tmp_pos_samples_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "As our train data is too skewed, we oversample positive samples in order to reduce the skew effect. \n",
    "The oversample consists on simple duplicate the positive samples as many times as the number of positive and negative samples are similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_oversampling():\n",
    "    global train_samples, train_labels, test_samples, test_labels, data_folds\n",
    "    \n",
    "    logging.info(\"Reducing data Skew by OverSampling...\")\n",
    "    for i in range(len(data_folds)):\n",
    "        tmp_pos_samples     = np.array(data_folds[i][\"labels\"]).sum()\n",
    "        tmp_neg_samples     = len(data_folds[i][\"labels\"]) - tmp_pos_samples\n",
    "\n",
    "        logging.info(\"<OS> {}-Fold: Initial {:4d} negative, {:4d} positive, {:2d}-1 ratio.\".format(i,tmp_neg_samples,tmp_pos_samples,tmp_neg_samples//tmp_pos_samples ))\n",
    "        logging.info(\"<OS> {}-Fold: Copying {:4d} times the positive samples.\".format(i,tmp_neg_samples//tmp_pos_samples))\n",
    "\n",
    "        tmp_pos_cpy_sample = []\n",
    "        tmp_pos_cpy_label  = []\n",
    "\n",
    "        for j in range(len(data_folds[i][\"labels\"])):\n",
    "            if data_folds[i][\"labels\"][j]==1:\n",
    "                tmp_pos_cpy_sample.append(data_folds[i][\"data\"][j])\n",
    "                tmp_pos_cpy_label.append(data_folds[i][\"labels\"][j])\n",
    "\n",
    "\n",
    "        for j in range(tmp_neg_samples//tmp_pos_samples-1):\n",
    "            data_folds[i][\"data\"].extend(tmp_pos_cpy_sample)\n",
    "            data_folds[i][\"labels\"].extend(tmp_pos_cpy_label)\n",
    "\n",
    "        tmp_pos_samples     = np.array(data_folds[i][\"labels\"]).sum()\n",
    "        tmp_neg_samples     = len(data_folds[i][\"labels\"]) - tmp_pos_samples\n",
    "\n",
    "        logging.info(\"<OS> {}-Fold: Final   {:4d} negative, {:4d} positive, {:2d}-1 ratio.\".format(i,tmp_neg_samples,tmp_pos_samples,tmp_neg_samples//tmp_pos_samples ))\n",
    "\n",
    "    logging.info(\"Oversampling... Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_run_cross_validation(iteration=0, parameters=None):\n",
    "    global train_labels, train_labels, test_samples, test_labels, data_folds\n",
    "    if config[\"app\"][\"PROC_MODE\"] == CONST_MODE_PARALLEL:\n",
    "        t0=time()\n",
    "        auc_per_fold =[]\n",
    "        scores=[]\n",
    "        fold_range = range (config[\"app\"][\"K_FOLDS\"])\n",
    "\n",
    "        args = [ (i, data_folds, PROJECT_DATA+\"crossvalidation/\", iteration, parameters) for i in fold_range ]\n",
    "\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            for number, result_score in zip(fold_range, executor.map(runParallelCrossValidation, *zip(*args) )):\n",
    "                auc_per_fold.append(result_score['auc'])\n",
    "                scores.append(result_score)\n",
    "\n",
    "        t1=time()\n",
    "        logging.info(\"Cross-Validation Ran in {:.3f} sec and achieved an average accuracy of {:.3f} with Min:{:.3f} and Max:{:.3f}\".format(t1-t0, np.mean(auc_per_fold), np.min(auc_per_fold), np.max(auc_per_fold) ))\n",
    "\n",
    "        return auc_per_fold, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"app\"][\"TEST_MAIN_PIPELINE\"]:\n",
    "    main_cross_validation_split()\n",
    "    main_oversampling()\n",
    "    scores = main_run_cross_validation() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **C** : float, optional (default=1.0):    Penalty parameter C of the error term.\n",
    "- **kernel** : string, optional (default=’rbf’)    Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples). \n",
    "- **degree** : int, optional (default=3)    Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "- **gamma** : float, optional (default=’auto’) Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.  Current default is ‘auto’ which uses 1 / n_features, if gamma='scale' is passed then it uses 1 / (n_features * X.var()) as value of gamma. The current default of gamma, ‘auto’, will change to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of ‘auto’ is used as a default indicating that no explicit value of gamma was passed.\n",
    "- **coef0** : float, optional (default=0.0)    Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
    "- **shrinking** : boolean, optional (default=True)    Whether to use the shrinking heuristic.\n",
    "- **probability** : boolean, optional (default=False)    Whether to enable probability estimates. This must be enabled prior to calling fit, and will slow down that method.\n",
    "- **tol** : float, optional (default=1e-3)    Tolerance for stopping criterion.\n",
    "- **class_weight** : {dict, ‘balanced’}, optional    Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "- **max_iter** : int, optional (default=-1)    Hard limit on iterations within solver, or -1 for no limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Configuration of Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"app\"][\"RUN_PARAM_TUNING\"]:\n",
    "    df_param_tuning = pd.read_csv(config[\"app\"][\"PARAM_FILE\"], comment=\"#\", skip_blank_lines=True)\n",
    "    df_param_tuning.reset_index(inplace=True)\n",
    "    parameter_list =[]\n",
    "    for i,row in df_param_tuning[:2].iterrows():\n",
    "        param ={}\n",
    "        for c in df_param_tuning.columns:\n",
    "            param[c]=row[c]\n",
    "        parameter_list.append(param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running test on Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"app\"][\"RUN_PARAM_TUNING\"]:\n",
    "\n",
    "    if not os.path.exists(config[\"app\"][\"PARAM_TUNING_FILE\"]): add_header=True\n",
    "    else: add_header=False\n",
    "        \n",
    "    param_tun_file = open(config[\"app\"][\"PARAM_TUNING_FILE\"],\"a\")\n",
    "    if add_header:\n",
    "        param_tun_file.write(\"param_set_id,metric,min,avg,max\\n\")\n",
    "    \n",
    "    # Same data distribution for all!\n",
    "    main_cross_validation_split()\n",
    "    main_oversampling()\n",
    "        \n",
    "    for param_set in parameter_list:\n",
    "\n",
    "        index = param_set.pop(\"index\")\n",
    "\n",
    "        if param_set[\"class_weight\"]==\"None\":\n",
    "            param_set.pop(\"class_weight\")\n",
    "\n",
    "        auc_scores, scores = main_run_cross_validation(parameters=param_set) \n",
    "        \n",
    "        for metric in ['auc','accuracy','recall','precision', 'F1']:\n",
    "            tmp=[]\n",
    "            for i in range(len(scores)):\n",
    "                tmp.append(scores[i][metric])\n",
    "            param_tun_file.write(\"{:03},{},{:.6f},{:.6f},{:.6f}\\n\".format(index,metric,np.min(tmp),np.mean(tmp),np.max(tmp)))\n",
    "\n",
    "    param_tun_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param_tuning_results = pd.read_csv(config[\"app\"][\"PARAM_TUNING_FILE\"], comment=\"#\", skip_blank_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests_metrics = []\n",
    "for metric in ['auc','accuracy','recall','precision', 'F1']:\n",
    "    #print(df_param_tuning_results[df_param_tuning_results[\"metric\"]==metric].sort_values(by=\"avg\", ascending=False)[:1])\n",
    "    bests_metrics.append(df_param_tuning_results[df_param_tuning_results[\"metric\"]==metric].sort_values(by=\"avg\", ascending=False)[:1][\"param_set_id\"].values[0])\n",
    "\n",
    "#bests_metrics\n",
    "#df_param_tuning_results[df_param_tuning_results[\"param_set_id\"].isin( bests_metrics)].sort_values(by=\"metric\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_param_tuning=pd.read_csv(config[\"app\"][\"PARAM_FILE\"], comment=\"#\", skip_blank_lines=True).reset_index()\n",
    "#df_param_tuning[df_param_tuning[\"index\"].isin(bests_metrics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Reading Function\n",
    "Read the parameter file and extract the parameter configuration for the model based on the config.file CHOOSEN_PARAMETER_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameters():\n",
    "    df_param_tuning=pd.read_csv(config[\"app\"][\"PARAM_FILE\"], comment=\"#\", skip_blank_lines=True).reset_index()\n",
    "\n",
    "    for i,row in df_param_tuning[df_param_tuning[\"index\"]==config[\"app\"][\"CHOOSEN_PARMETER_ID\"]].iterrows():\n",
    "        param ={}\n",
    "        for c in df_param_tuning.columns:\n",
    "            param[c]=row[c]\n",
    "    \n",
    "    #Removing index\n",
    "    index = param.pop(\"index\")\n",
    "\n",
    "    if param[\"class_weight\"]==\"None\":\n",
    "        param.pop(\"class_weight\")\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Full Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"app\"][\"RUN_CROSS_VAL_ITER\"]:\n",
    "    all_iter_auc = []\n",
    "    all_iter_t0 = time()\n",
    "\n",
    "    parameters = get_model_parameters()\n",
    "\n",
    "    for iter in range(config[\"app\"][\"K_FOLDS\"]):\n",
    "        logging.info(\"=== Running Iteration {} ===\".format(iter))\n",
    "        main_cross_validation_split()\n",
    "        main_oversampling()\n",
    "        tmp_auc, tmp_scores = main_run_cross_validation(iteration=iter, parameters=parameters)\n",
    "        all_iter_auc.extend(tmp_auc)\n",
    "\n",
    "    all_iter_t1 = time()\n",
    "    logging.info(\"=== Done With Iterations ===\")\n",
    "    logging.info(\"{}-fold Cross-Validation Iterations Ended. Process runtime {:3.2f}sec.\".format(config[\"app\"][\"K_FOLDS\"], all_iter_t1-all_iter_t0))\n",
    "    logging.info(\"{}-fold Cross-Validation Iterations Average AUC was {:3.5f}.\".format(          config[\"app\"][\"K_FOLDS\"], np.mean(all_iter_auc)))\n",
    "else:\n",
    "    logging.info(\"Skipping Cross-Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Training and Testing the Model with all Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split Proper Formating. \n",
    "```train_labels, train_labels``` will be the training set and \n",
    "```test_samples, test_labels``` will be the testing set.\n",
    "\n",
    "We reuse the ```main_cross_validation_split()``` method used for cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "As our train data is too skewed, we oversample positive samples in order to reduce the skew effect. \n",
    "The oversample consists on simple duplicate the positive samples as many times as the number of positive and negative samples are similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_test_oversampling():\n",
    "    global train_samples, train_labels, test_samples, test_labels\n",
    "    \n",
    "    logging.info(\"Reducing data Skew by OverSampling Train Data...\")\n",
    "    \n",
    "    tmp_pos_samples     = np.array(train_labels).sum()\n",
    "    tmp_neg_samples     = len(train_labels) - tmp_pos_samples\n",
    "\n",
    "    logging.info(\"<OS> Train: Initial {:4d} negative, {:4d} positive, {:2d}-1 ratio.\".format(tmp_neg_samples,tmp_pos_samples,tmp_neg_samples//tmp_pos_samples ))\n",
    "    logging.info(\"<OS> Train: Copying {:4d} times the positive samples.\".format(tmp_neg_samples//tmp_pos_samples))\n",
    "\n",
    "    tmp_pos_cpy_sample = []\n",
    "    tmp_pos_cpy_label  = []\n",
    "\n",
    "    for j in range(len(train_labels)):\n",
    "        if train_labels[j]==1:\n",
    "            tmp_pos_cpy_sample.append(train_samples[j])\n",
    "            tmp_pos_cpy_label.append(train_labels[j])\n",
    "\n",
    "\n",
    "    for j in range(tmp_neg_samples//tmp_pos_samples-1):\n",
    "        train_samples.extend(tmp_pos_cpy_sample)\n",
    "        train_labels.extend(tmp_pos_cpy_label)\n",
    "\n",
    "    tmp_pos_samples     = np.array(train_labels).sum()\n",
    "    tmp_neg_samples     = len(train_labels) - tmp_pos_samples\n",
    "\n",
    "    logging.info(\"<OS> Train: Final   {:4d} negative, {:4d} positive, {:2d}-1 ratio.\".format(tmp_neg_samples,tmp_pos_samples,tmp_neg_samples//tmp_pos_samples ))\n",
    "    logging.info(\"Oversampling... Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Model Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0        = time()\n",
    "\n",
    "parameters = get_model_parameters()\n",
    "\n",
    "main_cross_validation_split()\n",
    "main_train_test_oversampling()\n",
    "\n",
    "scores = runModelTrainTest(train_samples, train_labels, test_samples, test_labels, output_path=PROJECT_DATA+\"output/\", parameters=parameters)\n",
    "\n",
    "t1=time()\n",
    "\n",
    "logging.info(\"Main Program completed. Runtime was {:.3f} sec.\".format(t1-t0))\n",
    "logging.info(\"AUC:{:.3f}. Accuracy{:.3f}. Recall{:.3f}. Precision{:.3f}. F1{:.3f}.\".format(scores[\"auc\"],scores[\"accuracy\"],scores[\"recall\"],scores[\"precision\"],scores[\"F1\"]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"=== Done! Main Program Ended ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
